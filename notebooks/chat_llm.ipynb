{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import requests \n",
    "requests.adapters.DEFAULT_TIMEOUT = 60\n",
    "from huggingface_hub import InferenceClient\n",
    "import gradio as gr\n",
    "import random\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_key = os.environ['HF_API_KEY']\n",
    "hf_api_key_pro = os.environ['HF_API_KEY_PRO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">mistralai/Mistral-Nemo-Instruct-2407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Paris Olympics in 2024, officially known as the Games of the XXXIII Olympiad, will be held in Paris, France from July 26 to August 11, 2024. Here are some key points about these upcoming Olympics:\n",
      "\n",
      "1. **Venues**: Paris will use a combination of existing facilities, new venues specifically built for the Games, and temporary structures. Some iconic locations include the Eiffel Tower (for archery), the Stade de France (for athletics), and the Stade de Seine (for surfing, which makes its Olympic debut in 2024).\n",
      "\n",
      "2. **Sports**: The Paris Olympics will feature 41 sports, with 32 disciplines among them. In addition to the usual sports like swimming, gymnastics, and track and field, the 2024 Games will include breaking (also known as breakdancing), sport climbing, surfing, and skateboarding, which were introduced at the Tokyo Olympics in 2020.\n",
      "\n",
      "3. **Paralympics**: The Paris 2024 Paralympic Games will follow the Olympics, taking place from August 28 to September 8, 2024. They will also feature 41 sports.\n",
      "\n",
      "4. **Impact**: The Games are expected to generate around 400,000 jobs and contribute €11.3 billion to France's GDP in 2024, according to Paris 2024 organizers.\n",
      "\n",
      "5. **Green Initiative**: Paris 2024 is committed to being the most sustainable Olympics ever. The Games aim to be carbon neutral, with a focus on minimizing waste, using recycled materials, and increasing the use of renewable energy.\n",
      "\n",
      "6. **Medal Design**: The design of the medals for the Paris Olympics will incorporate materials from the cast-iron meteorite that fell in France in 1869.\n",
      "\n",
      "7. **Mascots**: The official mascot for the Paris 2024 Olympics is Phillip the Hypocamel, a humorously clumsy yet enchanting animal that is part phoenix and part camel.\n",
      "\n",
      "8. **Coronavirus Impact**: Despite some initial concerns, the Paris Olympics are expected to proceed as planned in 2024. However, organizers have stated that they are prepared to adjust plans as needed in response to the evolving COVID-19 situation.\n",
      "\n",
      "The Paris Olympics"
     ]
    }
   ],
   "source": [
    "client = InferenceClient(\n",
    "    \"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "    token=hf_api_key,\n",
    ")\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": \"What you know about Paris Olympics in 2024?\"}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">meta-llama/Meta-Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llama3 requires a pro subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Türkiye is Ankara."
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of Türkiye?\"\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "\tmax_tokens=256,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://13c61141caee85984b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://13c61141caee85984b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(input, slider):\n",
    "    messages = [{\"role\": \"user\", \"content\": input}]\n",
    "    output = \"\"\n",
    "    for message in client.chat_completion(messages=messages, max_tokens=slider, stream=True):\n",
    "        output += message.choices[0].delta.content\n",
    "    return output\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate,\n",
    "    inputs=[gr.Textbox(label=\"Prompt\"), \n",
    "            gr.Slider(label=\"Max new tokens\", value=20, maximum=1024, minimum=1)], \n",
    "    outputs=[gr.Textbox(label=\"Completion\")]\n",
    ")\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mock chatbot example with predefined messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://5fbdc1322af0ecb336.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5fbdc1322af0ecb336.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def respond(message, chat_history):\n",
    "        #No LLM here, just respond with a random pre-made message\n",
    "        bot_message = random.choice([\"Tell me more about it\", \n",
    "                                     \"Cool, but I'm not interested\", \n",
    "                                     \"Hmmmm, ok then\"]) \n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Running on local URL:  http://127.0.0.1:7866\n",
      "Running on public URL: https://edbe6fcbe67306d456.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://edbe6fcbe67306d456.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_chat_prompt(message, chat_history):\n",
    "    prompt = \"\"\n",
    "    for turn in chat_history:\n",
    "        user_message, bot_message = turn\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt\n",
    "\n",
    "def generate(formatted_prompt, slider):\n",
    "    messages = [{\"role\": \"user\", \"content\": formatted_prompt}]\n",
    "    output = \"\"\n",
    "    for message in client.chat_completion(messages=messages, max_tokens=slider, stream=True):\n",
    "        output += message.choices[0].delta.content\n",
    "        yield output  # Stream the output to update the chatbot immediately\n",
    "\n",
    "def respond(message, chat_history):\n",
    "    formatted_prompt = format_chat_prompt(message, chat_history)\n",
    "    bot_response_stream = generate(formatted_prompt, 1024)\n",
    "\n",
    "    # Append the user's message to chat history before generating the bot's response\n",
    "    chat_history.append((message, \"\"))\n",
    "\n",
    "    for bot_message in bot_response_stream:\n",
    "        chat_history[-1] = (message, bot_message)\n",
    "        yield \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240)  # Just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])  # Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Running on local URL:  http://127.0.0.1:7868\n",
      "Running on public URL: https://32310d8c56a53e1dac.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://32310d8c56a53e1dac.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_chat_prompt(message, chat_history, system_message):\n",
    "    prompt = system_message\n",
    "    for turn in chat_history:\n",
    "        user_message, bot_message = turn\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt\n",
    "\n",
    "def generate(formatted_prompt, slider, temperature):\n",
    "    messages = [{\"role\": \"user\", \"content\": formatted_prompt}]\n",
    "    output = \"\"\n",
    "    for message in client.chat_completion(messages=messages, max_tokens=slider, temperature=temperature, stream=True):\n",
    "        output += message.choices[0].delta.content\n",
    "        yield output  # Stream the output to update the chatbot immediately\n",
    "\n",
    "def respond(message, chat_history, system_message, temperature):\n",
    "    formatted_prompt = format_chat_prompt(message, chat_history, system_message)\n",
    "    bot_response_stream = generate(formatted_prompt, 1024, temperature)\n",
    "\n",
    "    # Append the user's message to chat history before generating the bot's response\n",
    "    chat_history.append((message, \"\"))\n",
    "\n",
    "    for bot_message in bot_response_stream:\n",
    "        chat_history[-1] = (message, bot_message)\n",
    "        yield \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240)  # Just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    with gr.Accordion(\"Advanced Options\", open=False):\n",
    "        system_msg = gr.Textbox(label=\"System Message\", value=\"You are a helpful assistant.\", placeholder=\"Set a custom system message.\")\n",
    "        temperature = gr.Slider(label=\"Temperature\", minimum=0.0, maximum=1.0, value=0.7, step=0.1)\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    \n",
    "    btn.click(respond, inputs=[msg, chatbot, system_msg, temperature], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot, system_msg, temperature], outputs=[msg, chatbot])  # Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
